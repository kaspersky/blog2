---
layout: post
title: Monkey week 3
excerpt: "GSoC 2014 Week 3."
modified: 2014-06-8
tags: [week3, gsoc2014]
comments: true
---

## Profiling

There are a few tools out there which can be used to profile a multithreaded Linux application, one notable example being valgrind. In order to test Monkey with valgrind 2 steps need to be performed:

1. Start Monkey using callgrind tool: `valgrind -tool=callgrind bin/monkey`

2. Inject http requests. I used for this *apache ab* tool, *httperf* and *monkey-bench*, all three yielding similar results

I set up Monkey to serve the static version of this blog as http content, and ran the tests invoking `bin/monkey` and the library. Somewhat expected, there was no differences regarding the performance aspect.

Below are some relevant call graphs exported by kcachegrind:

<figure>
<a href="{{ site.url }}/images/allcalls.png"><img src="{{ site.url }}/images/allcalls.png"></a>
<a href="{{ site.url }}/images/conn_read.png"><img src="{{ site.url }}/images/conn_read.png"></a>
<a href="{{ site.url }}/images/conn_write.png"><img src="{{ site.url }}/images/conn_write.png"></a>
<a href="{{ site.url }}/images/sched_get_conn.png"><img src="{{ site.url }}/images/sched_get_conn.png"></a>
</figure>

As an observation, `mk_sched_get_connection` is accessed a lot, performing a lookup in a red-black tree based on an integer (logarithmic time), this could be improved using a hashtable, however further analysis is needed.

## Perfomance counters

I implemented a simple and generic (mockup) [interface](https://github.com/kaspersky/monkey/blob/gsoc/src/include/mk_stats.h) for measuring how many times a function is called and its average execution time. Any internal Monkey function which runs in thread context (that is, called from a Monkey worker thread) can be monitored using:

{% highlight css %}
function name() {
    STATS_COUNTER_START(name);
    //...
    STATS_COUNTER_STOP(name);
}
{% endhighlight %}

Each worker (thread) stores a data structure which contains information related to function calls. The macros access this information and update it using `pthread_getspecific` and `clock_gettime` routines. On average these add a very small overhead (~100 ns), which is negligible as observed after rerunning the profiling test.

The stats can be later retrieved at any time using the `mklib_scheduler_worker_info` library call (this may need a mutex to prevent stat updates while fetching their values). I added a new [test](https://github.com/kaspersky/monkey/blob/gsoc/tests/lib/worker-info.c) for this.
